# DSLR: Datascience Ã— Logistic Regression  
### Harry Potter and a Data Scientist 

---

> Build a Hogwarts Sorting Hat using logistic regression and gradient descent.  
> Your mission: classify students by house from their course scores. Hogwarts is counting on you.

---

> âš ï¸ All analysis, conclusions and results are detailed in the Report.pdf file.

## â–ŒProject Overview

This project is a complete machine learning pipeline built from scratch â€” with **no external ML library** â€” to classify students into Hogwarts houses using **logistic regression**.

It's designed as part of **42 Paris' AI curriculum**, following the logic of `ft_linear_regression`, but extended to **multiclass classification** with:
- Data analysis,
- Visualization,
- Feature selection,
- Logistic regression (One-vs-All),
- Custom gradient descent (BGD, SGD, Mini-Batch).

<div align="center">

| Histogram | Correlation (Scatter plot) |
|:---:|:---:|
| <img src="https://github.com/user-attachments/assets/8513ed7f-deca-4705-b318-3a20b2b76ecd" alt="Histogram" width="500"> | <img src="https://github.com/user-attachments/assets/95ded32f-1aaf-42b6-8b60-81254d4e270f" alt="Correlation" width="500"> |

</div>

---

## â–ŒFeatures

âœ”ï¸ Full feature engineering and cleaning  
âœ”ï¸ Statistical description (mean, std, IQR, skewness, kurtosis...)  
âœ”ï¸ Data visualizations: Histogram, Scatter Plot, Pair Plot  
âœ”ï¸ Logistic regression classifier (One-vs-All)  
âœ”ï¸ Implements BGD, SGD and Mini-Batch GD  
âœ”ï¸ Output predictions in `houses.csv`  
âœ”ï¸ Manual handling of all statistics (no pandas, no scikit-learn)

---

## â–ŒHow it works

### â–  Method Used

The model uses **logistic regression** to estimate probabilities of each house, based on student features.  
One model per house is trained (One-vs-All), using gradient descent variants.

### â–  Hypothesis function

$$
h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}
$$

### â–  Cost function (Log-loss)

$$ J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \Big[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \Big] $$


### â–  Gradient formula

$$
\theta_j := \theta_j - \alpha \,\frac{1}{m} \sum_{i=1}^{m}
\big(h_\theta(x^{(i)}) - y^{(i)}\big)\, x_j^{(i)}
$$

â–ŒGetting Started
â–  Requirements
Python 3.x

No external ML packages allowed

You may use: `matplotlib`, `math`, `csv`, `os`

> âŒ No Pandas, Scikit-learn, Numpy, or functions like .mean(), .std(), etc.

### â–  Installation & Usage
1. Clone this repository

```bash
git clone https://github.com/ai-dg/dslr.git
cd dslr
```

2. Launch data visualization scripts:

```bash
python3 describe.py dataset/dataset_train.csv
python3 histogram.py dataset/dataset_train.csv
python3 scatter_plot.py dataset/dataset_train.csv
python3 pair_plot.py dataset/dataset_train.csv
```

3. Train the classifier:

```bash
python3 logreg_train.py dataset_train.csv
# Then choose method: BGD, SGD or M-Batch
```

4. Predict student houses from test set:

```bash
python3 logreg_predict.py dataset_test.csv
```
This generates houses.csv.

### â–ŒBonus Features

âœ”ï¸ Normalization of features  
âœ”ï¸ Training with multiple optimization strategies  
âœ”ï¸ Export trained Î¸ per house to JSON  
âœ”ï¸ Visualization of pairwise subject correlations   
âœ”ï¸ Calculation of precision vs actual labels  


### â–ŒExample Output

```bash
$ python3 logreg_train.py dataset_train.csv
Choose training method (BGD, SGD, M-Batch): SGD
ğŸ“‰ SGD average Log loss for Gryffindor: 0.045584
ğŸ“‰ SGD average Log loss for Hufflepuff: 0.058805
ğŸ“‰ SGD average Log loss for Ravenclaw: 0.069896
ğŸ“‰ SGD average Log loss for Slytherin: 0.048540

$ python3 logreg_predict.py dataset_test.csv
âœ… Results exported into houses.csv
âœ… Accuracy: 98.25%
```

### â–ŒSummary
This project teaches you how to:

- Manually analyze and visualize a dataset,
- Implement your own gradient descent,
- Train and apply a multiclass classifier,
- Evaluate and optimize performance (log-loss, accuracy),
- Work like a real data scientist... muggle or not.

### ğŸ“œ License
Project developed at **42 Paris** as part of the **AI/ML discovery curriculum.**
For educational purposes only.
Do not publish derivative work without referencing the original subject.

